{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasbidrahman10/Function-Stuff/blob/main/MIMIC_III_S2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe2537d",
      "metadata": {
        "id": "1fe2537d"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002a3c9e",
      "metadata": {
        "id": "002a3c9e",
        "outputId": "49dcc6a8-98a5-4f6f-f62d-16aecbb5d5cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vvb6TvpIP7Dk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5e743a-c268-4892-d208-9b9a71939a6a"
      },
      "id": "vvb6TvpIP7Dk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1353949"
      },
      "source": [
        "Assuming you've uploaded your dataset to Google Drive and mounted it, you would update the `base_path` variable like this:"
      ],
      "id": "e1353949"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb693667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f4f10c-aa8a-4678-b1f0-66c324b31c8e"
      },
      "source": [
        "base_path = \"/content/drive/MyDrive/mimic-iii-clinical-database-demo-1.4\" # Adjust this path to where your WESAD folder is located in Google Drive\n",
        "subject_id = \"S2\" # Change subject ID as needed\n",
        "file_path = os.path.join(base_path, \"mimic_timeseries.csv\")\n",
        "\n",
        "print(f\"New file path: {file_path}\")\n",
        "\n",
        "# Sampling and window settings\n",
        "TARGET_RATE = 1          # 1 sample per hour (assumed)\n",
        "WINDOW_HOURS = 24        # 24h window per patient\n",
        "WINDOW_STEPS = WINDOW_HOURS * TARGET_RATE  # 24 time steps\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n",
        "LR = 0.001\n",
        "\n",
        "# Feature columns (example vital signs & labs)\n",
        "FEATURE_COLUMNS = [\n",
        "    \"HR\",         # Heart rate\n",
        "    \"SBP\",        # Systolic blood pressure\n",
        "    \"DBP\",        # Diastolic blood pressure\n",
        "    \"MAP\",        # Mean arterial pressure\n",
        "    \"RR\",         # Respiratory rate\n",
        "    \"SpO2\",       # Oxygen saturation\n",
        "    \"Temp\",       # Temperature\n",
        "    \"Lactate\",    # Lactate level\n",
        "    \"Creatinine\", # Kidney function marker\n",
        "    \"BUN\",        # Blood urea nitrogen\n",
        "    \"GCS\"         # Glasgow Coma Scale\n",
        "]\n",
        "\n",
        "LABEL_COLUMN = \"mortality\"   # 0 = survived, 1 = died\n",
        "ID_COLUMN = \"patient_id\"\n",
        "TIME_COLUMN = \"time_hours\"   # Relative time index (0..T)\n"
      ],
      "id": "bb693667",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New file path: /content/drive/MyDrive/mimic-iii-clinical-database-demo-1.4/mimic_timeseries.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bbd597d"
      },
      "source": [
        "After updating the `base_path`, you would then re-run the cells that depend on `file_path` (e.g., the `load_and_process_data` call)."
      ],
      "id": "2bbd597d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e33b90f",
      "metadata": {
        "id": "4e33b90f"
      },
      "outputs": [],
      "source": [
        "def load_and_process_data(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"File not found at {file_path}\")\n",
        "\n",
        "    print(f\"Loading {file_path}...\")\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Basic checks\n",
        "    for col in [ID_COLUMN, TIME_COLUMN, LABEL_COLUMN]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Required column '{col}' not found in CSV.\")\n",
        "\n",
        "    for col in FEATURE_COLUMNS:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"Feature column '{col}' not found in CSV. \"\n",
        "                             f\"Either add it or remove from FEATURE_COLUMNS.\")\n",
        "\n",
        "    # Sort by patient and time to ensure sequence order\n",
        "    df = df.sort_values([ID_COLUMN, TIME_COLUMN])\n",
        "\n",
        "    # Handle missing values (simple strategy: forward-fill within patient, then back-fill)\n",
        "    df[FEATURE_COLUMNS] = df.groupby(ID_COLUMN)[FEATURE_COLUMNS].apply(\n",
        "        lambda group: group.ffill().bfill()\n",
        "    )\n",
        "\n",
        "    # Drop any remaining rows with missing values in features or labels\n",
        "    df = df.dropna(subset=FEATURE_COLUMNS + [LABEL_COLUMN])\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_patient_windows(df, window_steps):\n",
        "    \"\"\"\n",
        "    df: long-format dataframe with columns:\n",
        "        - patient_id, time_hours, label, feature columns\n",
        "    window_steps: how many time steps per patient window (e.g., 24 for 24 hours)\n",
        "    Returns:\n",
        "        X: (num_samples, num_channels, time_steps)\n",
        "        y: (num_samples,)\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    # Group by patient\n",
        "    grouped = df.groupby(ID_COLUMN)\n",
        "\n",
        "    for pid, group in grouped:\n",
        "        group = group.sort_values(TIME_COLUMN)\n",
        "\n",
        "        # If patient has at least `window_steps` time points, use the LAST window\n",
        "        if len(group) >= window_steps:\n",
        "            window = group.iloc[-window_steps:]  # last 24 hours\n",
        "        else:\n",
        "            # If shorter, pad at the start with the earliest available sample\n",
        "            deficit = window_steps - len(group)\n",
        "            first_row = group.iloc[0:1]\n",
        "            pad_block = pd.concat([first_row] * deficit, ignore_index=True)\n",
        "            window = pd.concat([pad_block, group], ignore_index=True)\n",
        "\n",
        "        # Extract features as numpy: (time_steps, num_channels)\n",
        "        features = window[FEATURE_COLUMNS].values.astype(np.float32)\n",
        "\n",
        "        # Take the label as the last known label (e.g., mortality outcome)\n",
        "        label = int(window[LABEL_COLUMN].iloc[-1])\n",
        "\n",
        "        # Transpose to (channels, time_steps) for Conv1d\n",
        "        X_list.append(features.T)\n",
        "        y_list.append(label)\n",
        "\n",
        "    X = np.stack(X_list, axis=0)  # (N, C, T)\n",
        "    y = np.array(y_list, dtype=np.int64)\n",
        "\n",
        "    print(f\"Created {len(X)} patient windows, shape: {X.shape}\")\n",
        "    print(f\"Label distribution: {np.bincount(y)}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def window_wise_normalization(X):\n",
        "    \"\"\"\n",
        "    Normalize each sample (window) across time.\n",
        "    X: (N, C, T)\n",
        "    \"\"\"\n",
        "    epsilon = 1e-6\n",
        "    means = X.mean(axis=2, keepdims=True)\n",
        "    stds = X.std(axis=2, keepdims=True)\n",
        "    X_norm = (X - means) / (stds + epsilon)\n",
        "    X_norm = X_norm.astype(np.float32)\n",
        "\n",
        "    print(f\"Window-wise normalized. Global mean: {X_norm.mean():.2f}, std: {X_norm.std():.2f}\")\n",
        "    return X_norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a00b1945",
      "metadata": {
        "id": "a00b1945",
        "outputId": "4d569521-96dc-435e-aad9-b5ef6c138979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Initialized for window size: 24\n"
          ]
        }
      ],
      "source": [
        "class WideScaleMIMICModel(nn.Module):\n",
        "    def __init__(self, input_channels=11, num_classes=2, window_size=24):\n",
        "        super(WideScaleMIMICModel, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Layer 1: Wide kernel to see broad temporal trends (e.g., over 5 hours)\n",
        "            nn.Conv1d(input_channels, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),  # 24 -> 12\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Layer 2: Finer details\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),  # 12 -> 6\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        final_timesteps = window_size // 4  # 24 -> 6\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * final_timesteps, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Calculate window size from your config\n",
        "window_size = WINDOW_HOURS * TARGET_RATE\n",
        "\n",
        "print(f\"Model Initialized for window size: {window_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "24499bbc",
      "metadata": {
        "id": "24499bbc",
        "outputId": "8793fa9a-4061-433a-8352-fc27bd2c3de4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n",
            "Training Complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1. Train\n",
        "print(\"Starting Training...\")\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = 100.0 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.2f}%\")\n",
        "\n",
        "print(\"Training Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c1b919ce",
      "metadata": {
        "id": "c1b919ce"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(model, loader, class_names=None):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    # Print Report\n",
        "    print(\"\\n--- Classification Report ---\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def attack_test(model, loader, noise_level):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Add Noise (Attack)\n",
        "            noise = torch.randn_like(inputs) * noise_level\n",
        "            attacked_inputs = inputs + noise\n",
        "\n",
        "            outputs = model(attacked_inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    return 100.0 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#main pipeline\n",
        "def main():\n",
        "    # 1. Load Data\n",
        "    df = load_and_process_data(file_path)\n",
        "\n",
        "    # 2. Create patient-level 24h windows\n",
        "    X, y = create_patient_windows(df, WINDOW_STEPS)\n",
        "\n",
        "    # 3. Window-wise normalization (per patient sequence)\n",
        "    X_norm = window_wise_normalization(X)\n",
        "\n",
        "    print(f\"Max Value in Data: {X_norm.max()}\")\n",
        "    print(f\"Min Value in Data: {X_norm.min()}\")\n",
        "    print(f\"Mean Value in Data: {X_norm.mean()}\")\n",
        "\n",
        "    # 4. Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_norm, y, test_size=0.2, random_state=42, stratify=y, shuffle=True\n",
        "    )\n",
        "\n",
        "    print(f\"Train samples: {len(X_train)} | Test samples: {len(X_test)}\")\n",
        "\n",
        "    # 5. Create DataLoaders\n",
        "    train_ds = TensorDataset(\n",
        "        torch.from_numpy(X_train), torch.from_numpy(y_train).long()\n",
        "    )\n",
        "    test_ds = TensorDataset(\n",
        "        torch.from_numpy(X_test), torch.from_numpy(y_test).long()\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # 6. Model init\n",
        "    input_channels = len(FEATURE_COLUMNS)\n",
        "    num_classes = 2\n",
        "    window_size = WINDOW_STEPS\n",
        "\n",
        "    model = WideScaleMIMICModel(\n",
        "        input_channels=input_channels,\n",
        "        num_classes=num_classes,\n",
        "        window_size=window_size,\n",
        "    ).to(device)\n",
        "\n",
        "    print(\n",
        "        f\"Model initialized with channels={input_channels}, \"\n",
        "        f\"window_size={window_size}\"\n",
        "    )\n",
        "\n",
        "    # 7. Class weights to handle imbalance\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(y_train),\n",
        "        y=y_train,\n",
        "    )\n",
        "    class_weights_tensor = torch.tensor(\n",
        "        class_weights, dtype=torch.float32\n",
        "    ).to(device)\n",
        "    print(f\"Using class weights: {class_weights_tensor}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    # 8. Train\n",
        "    train_model(model, train_loader, criterion, optimizer, EPOCHS)\n",
        "\n",
        "    # 9. Evaluate\n",
        "    class_names = [\"Survived\", \"Died\"]\n",
        "    evaluate_model(model, test_loader, class_names=class_names)\n",
        "\n",
        "    # 10. Intrusion / noise resilience test\n",
        "    print(\"\\n--- INTRUSION RESILIENCE RESULTS (MIMIC-III) ---\")\n",
        "    print(\"1. Clean Accuracy:\")\n",
        "    print(f\"   {attack_test(model, test_loader, 0.0):.2f}%\")\n",
        "\n",
        "    print(\"2. Accuracy under noise attack (noise_level=0.5):\")\n",
        "    print(f\"   {attack_test(model, test_loader, 0.5):.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "lkkHjlksnRFY",
        "outputId": "0d09be7c-e7cb-4bc8-b119-0e03926b09d0"
      },
      "id": "lkkHjlksnRFY",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "File not found at /content/drive/MyDrive/mimic-iii-clinical-database-demo-1.4/mimic_timeseries.csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2943346130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2943346130.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 1. Load Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 2. Create patient-level 24h windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-731112413.py\u001b[0m in \u001b[0;36mload_and_process_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File not found at {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading {file_path}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File not found at /content/drive/MyDrive/mimic-iii-clinical-database-demo-1.4/mimic_timeseries.csv"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ef6ad3",
      "metadata": {
        "id": "e7ef6ad3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}